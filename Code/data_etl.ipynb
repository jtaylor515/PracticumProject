{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert business JSON file to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to CSV file: /Users/jeffreytaylor/Documents/Emory/CS370/PracticumProject/Data/yelp_dataset/business.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import csv\n",
    "\n",
    "# Replace 'your_file.json' with the actual filename of your JSON data\n",
    "filename = '/Users/jeffreytaylor/Documents/Emory/CS370/PracticumProject/Data/yelp_dataset/yelp_academic_dataset_business.json'\n",
    "\n",
    "# Read the JSON data into a pandas DataFrame\n",
    "df = pd.read_json(filename, lines=True)\n",
    "\n",
    "# Define the filename for the output CSV file\n",
    "output_filename = '/Users/jeffreytaylor/Documents/Emory/CS370/PracticumProject/Data/yelp_dataset/business.csv'\n",
    "\n",
    "# Only for Business table\n",
    "# Convert attributes column in-place\n",
    "df['attributes'] = df['attributes'].apply(json.dumps)\n",
    "\n",
    "# Convert hours column in-place\n",
    "df['hours'] = df['hours'].apply(json.dumps)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(output_filename, index=False, quoting=csv.QUOTE_ALL)\n",
    "\n",
    "print(f'Data successfully written to CSV file: {output_filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>name</th>\n",
       "      <th>address</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>postal_code</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>stars</th>\n",
       "      <th>review_count</th>\n",
       "      <th>is_open</th>\n",
       "      <th>attributes</th>\n",
       "      <th>categories</th>\n",
       "      <th>hours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [business_id, name, address, city, state, postal_code, latitude, longitude, stars, review_count, is_open, attributes, categories, hours]\n",
       "Index: []"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "df2 = pd.read_csv('/Users/jeffreytaylor/Documents/Emory/CS370/PracticumProject/Data/yelp_dataset/business.csv')\n",
    "\n",
    "df2.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Business SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing SQL Query AI Bot User\n",
    "import psycopg2\n",
    "\n",
    "# Connect to the PostgreSQL server\n",
    "conn = psycopg2.connect(\n",
    "    host=\"envitas-db.cj44ees44tpz.us-east-1.rds.amazonaws.com\",\n",
    "    database=\"postgres\",\n",
    "    user=\"bot\",\n",
    "    password=\"envitas123\"\n",
    ")\n",
    "\n",
    "# Create a cursor object to interact with the database\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Set the search path to 'yelp' schema\n",
    "cur.execute(\"SET search_path TO yelp\")\n",
    "\n",
    "# Execute the query\n",
    "query = \"SELECT name, address, city, state, stars, review_count FROM raw_business LIMIT 10;\"\n",
    "cur.execute(query)\n",
    "\n",
    "# Fetch all the results\n",
    "results = cur.fetchall()\n",
    "\n",
    "# Print the results\n",
    "for row in results:\n",
    "    print(row)\n",
    "\n",
    "# Close the cursor and connection\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tip Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Tip JSON to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to CSV file: /Users/jeffreytaylor/Documents/Emory/CS370/PracticumProject/Data/yelp_dataset/tip.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "\n",
    "# Replace 'your_file.json' with the actual filename of your JSON data\n",
    "filename = '/Users/jeffreytaylor/Documents/Emory/CS370/PracticumProject/Data/yelp_dataset/yelp_academic_dataset_tip.json'\n",
    "\n",
    "# Read the JSON data into a pandas DataFrame\n",
    "df = pd.read_json(filename, lines=True)\n",
    "\n",
    "# Define the filename for the output CSV file\n",
    "output_filename = '/Users/jeffreytaylor/Documents/Emory/CS370/PracticumProject/Data/yelp_dataset/tip.csv'\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(output_filename, index=False, quoting=csv.QUOTE_ALL)\n",
    "\n",
    "print(f'Data successfully written to CSV file: {output_filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>compliment_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AGNUgVwnZUey3gcPCJ76iw</td>\n",
       "      <td>3uLgwr0qeCNMjKenHJwPGQ</td>\n",
       "      <td>Avengers time with the ladies.</td>\n",
       "      <td>2012-05-18 02:17:21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NBN4MgHP9D3cw--SnauTkA</td>\n",
       "      <td>QoezRbYQncpRqyrLH6Iqjg</td>\n",
       "      <td>They have lots of good deserts and tasty cuban...</td>\n",
       "      <td>2013-02-05 18:35:10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-copOvldyKh1qr-vzkDEvw</td>\n",
       "      <td>MYoRNLb5chwjQe3c_k37Gg</td>\n",
       "      <td>It's open even when you think it isn't</td>\n",
       "      <td>2013-08-18 00:56:08</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FjMQVZjSqY8syIO-53KFKw</td>\n",
       "      <td>hV-bABTK-glh5wj31ps_Jw</td>\n",
       "      <td>Very decent fried chicken</td>\n",
       "      <td>2017-06-27 23:05:38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ld0AperBXk1h6UbqmM80zw</td>\n",
       "      <td>_uN0OudeJ3Zl_tf6nxg5ww</td>\n",
       "      <td>Appetizers.. platter special for lunch</td>\n",
       "      <td>2012-10-06 19:43:09</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>trf3Qcz8qvCDKXiTgjUcEg</td>\n",
       "      <td>7Rm9Ba50bw23KTA8RedZYg</td>\n",
       "      <td>Chili Cup + Single Cheeseburger with onion, pi...</td>\n",
       "      <td>2012-03-13 04:00:52</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SMGAlRjyfuYu-c-22zIyOg</td>\n",
       "      <td>kH-0iXqkL7b8UXNpguBMKg</td>\n",
       "      <td>Saturday, Dec 7th 2013, ride Patco's Silver Sl...</td>\n",
       "      <td>2013-12-03 23:42:15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>YVBB9g23nuVJ0u44zK0pSA</td>\n",
       "      <td>jtri188kuhe_AuEOJ51U_A</td>\n",
       "      <td>This is probably the best place in the cool Sp...</td>\n",
       "      <td>2016-11-22 22:14:58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>VL12EhEdT4OWqGq0nIqkzw</td>\n",
       "      <td>xODBZmX4EmlVvbqtKN7YKg</td>\n",
       "      <td>Tacos</td>\n",
       "      <td>2012-07-27 01:48:24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4ay-fdVks5WMerYL_htkGQ</td>\n",
       "      <td>pICJRcyqW1cF96Q3XhLSbw</td>\n",
       "      <td>Starbucks substitute in boring downtown Tampa....</td>\n",
       "      <td>2012-06-09 22:57:04</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  user_id             business_id  \\\n",
       "0  AGNUgVwnZUey3gcPCJ76iw  3uLgwr0qeCNMjKenHJwPGQ   \n",
       "1  NBN4MgHP9D3cw--SnauTkA  QoezRbYQncpRqyrLH6Iqjg   \n",
       "2  -copOvldyKh1qr-vzkDEvw  MYoRNLb5chwjQe3c_k37Gg   \n",
       "3  FjMQVZjSqY8syIO-53KFKw  hV-bABTK-glh5wj31ps_Jw   \n",
       "4  ld0AperBXk1h6UbqmM80zw  _uN0OudeJ3Zl_tf6nxg5ww   \n",
       "5  trf3Qcz8qvCDKXiTgjUcEg  7Rm9Ba50bw23KTA8RedZYg   \n",
       "6  SMGAlRjyfuYu-c-22zIyOg  kH-0iXqkL7b8UXNpguBMKg   \n",
       "7  YVBB9g23nuVJ0u44zK0pSA  jtri188kuhe_AuEOJ51U_A   \n",
       "8  VL12EhEdT4OWqGq0nIqkzw  xODBZmX4EmlVvbqtKN7YKg   \n",
       "9  4ay-fdVks5WMerYL_htkGQ  pICJRcyqW1cF96Q3XhLSbw   \n",
       "\n",
       "                                                text                 date  \\\n",
       "0                     Avengers time with the ladies.  2012-05-18 02:17:21   \n",
       "1  They have lots of good deserts and tasty cuban...  2013-02-05 18:35:10   \n",
       "2             It's open even when you think it isn't  2013-08-18 00:56:08   \n",
       "3                          Very decent fried chicken  2017-06-27 23:05:38   \n",
       "4             Appetizers.. platter special for lunch  2012-10-06 19:43:09   \n",
       "5  Chili Cup + Single Cheeseburger with onion, pi...  2012-03-13 04:00:52   \n",
       "6  Saturday, Dec 7th 2013, ride Patco's Silver Sl...  2013-12-03 23:42:15   \n",
       "7  This is probably the best place in the cool Sp...  2016-11-22 22:14:58   \n",
       "8                                              Tacos  2012-07-27 01:48:24   \n",
       "9  Starbucks substitute in boring downtown Tampa....  2012-06-09 22:57:04   \n",
       "\n",
       "   compliment_count  \n",
       "0                 0  \n",
       "1                 0  \n",
       "2                 0  \n",
       "3                 0  \n",
       "4                 0  \n",
       "5                 0  \n",
       "6                 0  \n",
       "7                 0  \n",
       "8                 0  \n",
       "9                 0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "df2 = pd.read_csv('/Users/jeffreytaylor/Documents/Emory/CS370/PracticumProject/yelp_dataset/tip.csv')\n",
    "\n",
    "df2.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Tip SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('VaO-VW3e1kARkU9bP1E7Fw', 'Roasted oysters here the BEST bite of NOLA!  Hauntingly good!!!', '0')\n",
      "('V_jy9Aemc3kjznqhbsu_Dg', 'The service blows', '0')\n",
      "('yGvaIruh-JIm8FFZLCWoYg', 'Big portions, so come hungry!!!', '0')\n",
      "('tz5rytGzU6DsZpHUxTRhUw', 'They use Caviar for delivery', '0')\n",
      "('92KkiumP0bmvDM-1eyHWKg', 'Nino and his food is the man!!!', '0')\n",
      "('hVRebPv7EKMUGVV2m5IKaQ', 'Thank goodness iced coffee is only $1', '0')\n",
      "('4IIAxlEz4HLZbAH6DzFnQw', 'The fried Okra is a MUST!', '0')\n",
      "('jOaJ4PFGhGiM8fhuYcYFRg', 'Dont get much better than this. The king of Cheeseteaks', '0')\n",
      "('yQZpO60yknDkCPm5OG0IVA', 'Very nice. Rice bowls - try the Teriyaki Shrimp. Good happy hour specials.', '0')\n",
      "('3FKIev7ZB_KE6XHL9sUJCg', 'Good food, good bartender and cool deco', '0')\n"
     ]
    }
   ],
   "source": [
    "# Testing SQL Query AI Bot User\n",
    "import psycopg2\n",
    "\n",
    "# Connect to the PostgreSQL server\n",
    "conn = psycopg2.connect(\n",
    "    host=\"envitas-db.cvga08s8c0bm.us-west-1.rds.amazonaws.com\",\n",
    "    database=\"postgres\",\n",
    "    user=\"bot\",\n",
    "    password=\"envitas\"\n",
    ")\n",
    "\n",
    "# Create a cursor object to interact with the database\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Set the search path to 'yelp' schema\n",
    "cur.execute(\"SET search_path TO yelp\")\n",
    "\n",
    "# Execute the query\n",
    "query = \"SELECT business_id, text, complement_count FROM raw_tip LIMIT 10;\"\n",
    "cur.execute(query)\n",
    "\n",
    "# Fetch all the results\n",
    "results = cur.fetchall()\n",
    "\n",
    "# Print the results\n",
    "for row in results:\n",
    "    print(row)\n",
    "\n",
    "# Close the cursor and connection\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to CSV file: /Users/jeffreytaylor/Documents/Emory/CS370/PracticumProject/Data/yelp_dataset/user.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "\n",
    "# Replace 'your_file.json' with the actual filename of your JSON data\n",
    "filename = '/Users/jeffreytaylor/Documents/Emory/CS370/PracticumProject/Data/yelp_dataset/yelp_academic_dataset_user.json'\n",
    "\n",
    "# Read the JSON data into a pandas DataFrame\n",
    "df = pd.read_json(filename, lines=True)\n",
    "\n",
    "# Define the filename for the output CSV file\n",
    "output_filename = '/Users/jeffreytaylor/Documents/Emory/CS370/PracticumProject/Data/yelp_dataset/user.csv'\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(output_filename, index=False, quoting=csv.QUOTE_ALL)\n",
    "\n",
    "print(f'Data successfully written to CSV file: {output_filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qd/rhs7cnfj66l4p8v_yrkw8n4m0000gn/T/ipykernel_76534/3175553029.py:2: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df2 = pd.read_csv('/Users/jeffreytaylor/Documents/Emory/CS370/PracticumProject/yelp_dataset/user.csv')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>name</th>\n",
       "      <th>review_count</th>\n",
       "      <th>yelping_since</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>elite</th>\n",
       "      <th>friends</th>\n",
       "      <th>fans</th>\n",
       "      <th>...</th>\n",
       "      <th>compliment_more</th>\n",
       "      <th>compliment_profile</th>\n",
       "      <th>compliment_cute</th>\n",
       "      <th>compliment_list</th>\n",
       "      <th>compliment_note</th>\n",
       "      <th>compliment_plain</th>\n",
       "      <th>compliment_cool</th>\n",
       "      <th>compliment_funny</th>\n",
       "      <th>compliment_writer</th>\n",
       "      <th>compliment_photos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>qVc8ODYU5SZjKXVBgXdI7w</td>\n",
       "      <td>Walker</td>\n",
       "      <td>585</td>\n",
       "      <td>2007-01-25 16:47:26</td>\n",
       "      <td>7217</td>\n",
       "      <td>1259</td>\n",
       "      <td>5994</td>\n",
       "      <td>2007</td>\n",
       "      <td>NSCy54eWehBJyZdG2iE84w, pe42u7DcCH2QmI81NX-8qA...</td>\n",
       "      <td>267</td>\n",
       "      <td>...</td>\n",
       "      <td>65</td>\n",
       "      <td>55</td>\n",
       "      <td>56</td>\n",
       "      <td>18</td>\n",
       "      <td>232</td>\n",
       "      <td>844</td>\n",
       "      <td>467</td>\n",
       "      <td>467</td>\n",
       "      <td>239</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>j14WgRoU_-2ZE1aw1dXrJg</td>\n",
       "      <td>Daniel</td>\n",
       "      <td>4333</td>\n",
       "      <td>2009-01-25 04:35:42</td>\n",
       "      <td>43091</td>\n",
       "      <td>13066</td>\n",
       "      <td>27281</td>\n",
       "      <td>2009,2010,2011,2012,2013,2014,2015,2016,2017,2...</td>\n",
       "      <td>ueRPE0CX75ePGMqOFVj6IQ, 52oH4DrRvzzl8wh5UXyU0A...</td>\n",
       "      <td>3138</td>\n",
       "      <td>...</td>\n",
       "      <td>264</td>\n",
       "      <td>184</td>\n",
       "      <td>157</td>\n",
       "      <td>251</td>\n",
       "      <td>1847</td>\n",
       "      <td>7054</td>\n",
       "      <td>3131</td>\n",
       "      <td>3131</td>\n",
       "      <td>1521</td>\n",
       "      <td>1946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2WnXYQFK0hXEoTxPtV2zvg</td>\n",
       "      <td>Steph</td>\n",
       "      <td>665</td>\n",
       "      <td>2008-07-25 10:41:00</td>\n",
       "      <td>2086</td>\n",
       "      <td>1010</td>\n",
       "      <td>1003</td>\n",
       "      <td>2009,2010,2011,2012,2013</td>\n",
       "      <td>LuO3Bn4f3rlhyHIaNfTlnA, j9B4XdHUhDfTKVecyWQgyA...</td>\n",
       "      <td>52</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>66</td>\n",
       "      <td>96</td>\n",
       "      <td>119</td>\n",
       "      <td>119</td>\n",
       "      <td>35</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SZDeASXq7o05mMNLshsdIA</td>\n",
       "      <td>Gwen</td>\n",
       "      <td>224</td>\n",
       "      <td>2005-11-29 04:38:33</td>\n",
       "      <td>512</td>\n",
       "      <td>330</td>\n",
       "      <td>299</td>\n",
       "      <td>2009,2010,2011</td>\n",
       "      <td>enx1vVPnfdNUdPho6PH_wg, 4wOcvMLtU6a9Lslggq74Vg...</td>\n",
       "      <td>28</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>16</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hA5lMy-EnncsH4JoR-hFGQ</td>\n",
       "      <td>Karen</td>\n",
       "      <td>79</td>\n",
       "      <td>2007-01-05 19:40:59</td>\n",
       "      <td>29</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PBK4q9KEEBHhFvSXCUirIw, 3FWPpM7KU1gXeOM_ZbYMbA...</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>q_QQ5kBBwlCcbL1s4NVK3g</td>\n",
       "      <td>Jane</td>\n",
       "      <td>1221</td>\n",
       "      <td>2005-03-14 20:26:35</td>\n",
       "      <td>14953</td>\n",
       "      <td>9940</td>\n",
       "      <td>11211</td>\n",
       "      <td>2006,2007,2008,2009,2010,2011,2012,2013,2014</td>\n",
       "      <td>xBDpTUbai0DXrvxCe3X16Q, 7GPNBO496aecrjJfW6UWtg...</td>\n",
       "      <td>1357</td>\n",
       "      <td>...</td>\n",
       "      <td>163</td>\n",
       "      <td>191</td>\n",
       "      <td>361</td>\n",
       "      <td>147</td>\n",
       "      <td>1212</td>\n",
       "      <td>5696</td>\n",
       "      <td>2543</td>\n",
       "      <td>2543</td>\n",
       "      <td>815</td>\n",
       "      <td>323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cxuxXkcihfCbqt5Byrup8Q</td>\n",
       "      <td>Rob</td>\n",
       "      <td>12</td>\n",
       "      <td>2009-02-24 03:09:06</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HDAQ74AEznP-YsMk1B14CA, 6A6-aIX7fg_zRy9MiE6YyQ...</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>E9kcWJdJUHuTKfQurPljwA</td>\n",
       "      <td>Mike</td>\n",
       "      <td>358</td>\n",
       "      <td>2008-12-11 22:11:56</td>\n",
       "      <td>399</td>\n",
       "      <td>102</td>\n",
       "      <td>143</td>\n",
       "      <td>NaN</td>\n",
       "      <td>y2GyxJF5VQWohxgw_GR7Jw, 0NRn4eY3JWN0IFqvOSa5gA...</td>\n",
       "      <td>23</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>lO1iq-f75hnPNZkTy3Zerg</td>\n",
       "      <td>Rachelle</td>\n",
       "      <td>40</td>\n",
       "      <td>2008-12-29 22:40:56</td>\n",
       "      <td>109</td>\n",
       "      <td>40</td>\n",
       "      <td>46</td>\n",
       "      <td>NaN</td>\n",
       "      <td>tOQDlz36rI__SOsbL-HCag, 83Xb0PPBwZiG2c_fLpZgAw...</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AUi8MPWJ0mLkMfwbui27lg</td>\n",
       "      <td>John</td>\n",
       "      <td>109</td>\n",
       "      <td>2010-01-07 18:32:04</td>\n",
       "      <td>154</td>\n",
       "      <td>20</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>gy5fWeSv3Gamuq9Ox4MV4g, lMr3LWU6kPFLTmCpDkACxg...</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  user_id      name  review_count        yelping_since  \\\n",
       "0  qVc8ODYU5SZjKXVBgXdI7w    Walker           585  2007-01-25 16:47:26   \n",
       "1  j14WgRoU_-2ZE1aw1dXrJg    Daniel          4333  2009-01-25 04:35:42   \n",
       "2  2WnXYQFK0hXEoTxPtV2zvg     Steph           665  2008-07-25 10:41:00   \n",
       "3  SZDeASXq7o05mMNLshsdIA      Gwen           224  2005-11-29 04:38:33   \n",
       "4  hA5lMy-EnncsH4JoR-hFGQ     Karen            79  2007-01-05 19:40:59   \n",
       "5  q_QQ5kBBwlCcbL1s4NVK3g      Jane          1221  2005-03-14 20:26:35   \n",
       "6  cxuxXkcihfCbqt5Byrup8Q       Rob            12  2009-02-24 03:09:06   \n",
       "7  E9kcWJdJUHuTKfQurPljwA      Mike           358  2008-12-11 22:11:56   \n",
       "8  lO1iq-f75hnPNZkTy3Zerg  Rachelle            40  2008-12-29 22:40:56   \n",
       "9  AUi8MPWJ0mLkMfwbui27lg      John           109  2010-01-07 18:32:04   \n",
       "\n",
       "   useful  funny   cool                                              elite  \\\n",
       "0    7217   1259   5994                                               2007   \n",
       "1   43091  13066  27281  2009,2010,2011,2012,2013,2014,2015,2016,2017,2...   \n",
       "2    2086   1010   1003                           2009,2010,2011,2012,2013   \n",
       "3     512    330    299                                     2009,2010,2011   \n",
       "4      29     15      7                                                NaN   \n",
       "5   14953   9940  11211       2006,2007,2008,2009,2010,2011,2012,2013,2014   \n",
       "6       6      1      0                                                NaN   \n",
       "7     399    102    143                                                NaN   \n",
       "8     109     40     46                                                NaN   \n",
       "9     154     20     23                                                NaN   \n",
       "\n",
       "                                             friends  fans  ...  \\\n",
       "0  NSCy54eWehBJyZdG2iE84w, pe42u7DcCH2QmI81NX-8qA...   267  ...   \n",
       "1  ueRPE0CX75ePGMqOFVj6IQ, 52oH4DrRvzzl8wh5UXyU0A...  3138  ...   \n",
       "2  LuO3Bn4f3rlhyHIaNfTlnA, j9B4XdHUhDfTKVecyWQgyA...    52  ...   \n",
       "3  enx1vVPnfdNUdPho6PH_wg, 4wOcvMLtU6a9Lslggq74Vg...    28  ...   \n",
       "4  PBK4q9KEEBHhFvSXCUirIw, 3FWPpM7KU1gXeOM_ZbYMbA...     1  ...   \n",
       "5  xBDpTUbai0DXrvxCe3X16Q, 7GPNBO496aecrjJfW6UWtg...  1357  ...   \n",
       "6  HDAQ74AEznP-YsMk1B14CA, 6A6-aIX7fg_zRy9MiE6YyQ...     1  ...   \n",
       "7  y2GyxJF5VQWohxgw_GR7Jw, 0NRn4eY3JWN0IFqvOSa5gA...    23  ...   \n",
       "8  tOQDlz36rI__SOsbL-HCag, 83Xb0PPBwZiG2c_fLpZgAw...     7  ...   \n",
       "9  gy5fWeSv3Gamuq9Ox4MV4g, lMr3LWU6kPFLTmCpDkACxg...     4  ...   \n",
       "\n",
       "   compliment_more  compliment_profile  compliment_cute  compliment_list  \\\n",
       "0               65                  55               56               18   \n",
       "1              264                 184              157              251   \n",
       "2               13                  10               17                3   \n",
       "3                4                   1                6                2   \n",
       "4                1                   0                0                0   \n",
       "5              163                 191              361              147   \n",
       "6                0                   0                0                0   \n",
       "7                7                   2                0                0   \n",
       "8                0                   0                3                0   \n",
       "9                0                   0                0                0   \n",
       "\n",
       "   compliment_note  compliment_plain  compliment_cool  compliment_funny  \\\n",
       "0              232               844              467               467   \n",
       "1             1847              7054             3131              3131   \n",
       "2               66                96              119               119   \n",
       "3               12                16               26                26   \n",
       "4                1                 1                0                 0   \n",
       "5             1212              5696             2543              2543   \n",
       "6                0                 1                0                 0   \n",
       "7                8                 6               12                12   \n",
       "8                3                 4                5                 5   \n",
       "9                1                 6                3                 3   \n",
       "\n",
       "   compliment_writer  compliment_photos  \n",
       "0                239                180  \n",
       "1               1521               1946  \n",
       "2                 35                 18  \n",
       "3                 10                  9  \n",
       "4                  0                  0  \n",
       "5                815                323  \n",
       "6                  0                  0  \n",
       "7                  5                  0  \n",
       "8                  3                  1  \n",
       "9                  0                  0  \n",
       "\n",
       "[10 rows x 22 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "df2 = pd.read_csv('/Users/jeffreytaylor/Documents/Emory/CS370/PracticumProject/yelp_dataset/user.csv')\n",
    "\n",
    "df2.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Walker', '2007-01-25 16:47:26', '585', '7217', '1259', '5994', '3.91')\n",
      "('Daniel', '2009-01-25 04:35:42', '4333', '43091', '13066', '27281', '3.74')\n",
      "('Steph', '2008-07-25 10:41:00', '665', '2086', '1010', '1003', '3.32')\n",
      "('Gwen', '2005-11-29 04:38:33', '224', '512', '330', '299', '4.27')\n",
      "('Karen', '2007-01-05 19:40:59', '79', '29', '15', '7', '3.54')\n",
      "('Jane', '2005-03-14 20:26:35', '1221', '14953', '9940', '11211', '3.85')\n",
      "('Rob', '2009-02-24 03:09:06', '12', '6', '1', '0', '2.75')\n",
      "('Mike', '2008-12-11 22:11:56', '358', '399', '102', '143', '3.73')\n",
      "('Rachelle', '2008-12-29 22:40:56', '40', '109', '40', '46', '4.04')\n",
      "('John', '2010-01-07 18:32:04', '109', '154', '20', '23', '3.4')\n"
     ]
    }
   ],
   "source": [
    "# Testing SQL Query AI Bot User\n",
    "import psycopg2\n",
    "\n",
    "# Connect to the PostgreSQL server\n",
    "conn = psycopg2.connect(\n",
    "    host=\"envitas-db.cvga08s8c0bm.us-west-1.rds.amazonaws.com\",\n",
    "    database=\"postgres\",\n",
    "    user=\"bot\",\n",
    "    password=\"envitas\"\n",
    ")\n",
    "\n",
    "# Create a cursor object to interact with the database\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Set the search path to 'yelp' schema\n",
    "cur.execute(\"SET search_path TO yelp\")\n",
    "\n",
    "# Execute the query\n",
    "query = \"SELECT name, yelping_since, review_count, useful, funny, cool, average_stars FROM raw_user LIMIT 10;\"\n",
    "cur.execute(query)\n",
    "\n",
    "# Fetch all the results\n",
    "results = cur.fetchall()\n",
    "\n",
    "# Print the results\n",
    "for row in results:\n",
    "    print(row)\n",
    "\n",
    "# Close the cursor and connection\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkin Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to CSV file: /Users/jeffreytaylor/Documents/Emory/CS370/PracticumProject/Data/yelp_dataset/checkin.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "\n",
    "# Replace 'your_file.json' with the actual filename of your JSON data\n",
    "filename = '/Users/jeffreytaylor/Documents/Emory/CS370/PracticumProject/Data/yelp_dataset/yelp_academic_dataset_checkin.json'\n",
    "\n",
    "# Read the JSON data into a pandas DataFrame\n",
    "df = pd.read_json(filename, lines=True)\n",
    "\n",
    "# Define the filename for the output CSV file\n",
    "output_filename = '/Users/jeffreytaylor/Documents/Emory/CS370/PracticumProject/Data/yelp_dataset/checkin.csv'\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(output_filename, index=False, quoting= csv.QUOTE_ALL)\n",
    "\n",
    "print(f'Data successfully written to CSV file: {output_filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>---kPU91CF4Lq2-WlRu9Lw</td>\n",
       "      <td>2020-03-13 21:10:56, 2020-06-02 22:18:06, 2020...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>--0iUa4sNDFiZFrAdIWhZQ</td>\n",
       "      <td>2010-09-13 21:43:09, 2011-05-04 23:08:15, 2011...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>--30_8IhuyMHbSOcNWd6DQ</td>\n",
       "      <td>2013-06-14 23:29:17, 2014-08-13 23:20:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>--7PUidqRWpRSpXebiyxTg</td>\n",
       "      <td>2011-02-15 17:12:00, 2011-07-28 02:46:10, 2012...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>--7jw19RH9JKXgFohspgQw</td>\n",
       "      <td>2014-04-21 20:42:11, 2014-04-28 21:04:46, 2014...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>--8IbOsAAxjKRoYsBFL-PA</td>\n",
       "      <td>2015-06-06 01:03:19, 2015-07-29 16:50:58, 2015...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>--9osgUCSDUWUkoTLdvYhQ</td>\n",
       "      <td>2015-06-13 02:00:57, 2015-07-04 00:44:09, 2015...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>--ARBQr1WMsTWiwOKOj-FQ</td>\n",
       "      <td>2014-12-12 00:44:23, 2015-01-09 00:19:52, 2015...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>--FWWsIwxRwuw9vIMImcQg</td>\n",
       "      <td>2010-09-11 16:28:39, 2010-12-22 21:14:19, 2011...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>--FcbSxK1AoEtEAxOgBaCw</td>\n",
       "      <td>2017-08-18 19:43:50, 2017-10-07 22:38:38, 2017...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id                                               date\n",
       "0  ---kPU91CF4Lq2-WlRu9Lw  2020-03-13 21:10:56, 2020-06-02 22:18:06, 2020...\n",
       "1  --0iUa4sNDFiZFrAdIWhZQ  2010-09-13 21:43:09, 2011-05-04 23:08:15, 2011...\n",
       "2  --30_8IhuyMHbSOcNWd6DQ           2013-06-14 23:29:17, 2014-08-13 23:20:22\n",
       "3  --7PUidqRWpRSpXebiyxTg  2011-02-15 17:12:00, 2011-07-28 02:46:10, 2012...\n",
       "4  --7jw19RH9JKXgFohspgQw  2014-04-21 20:42:11, 2014-04-28 21:04:46, 2014...\n",
       "5  --8IbOsAAxjKRoYsBFL-PA  2015-06-06 01:03:19, 2015-07-29 16:50:58, 2015...\n",
       "6  --9osgUCSDUWUkoTLdvYhQ  2015-06-13 02:00:57, 2015-07-04 00:44:09, 2015...\n",
       "7  --ARBQr1WMsTWiwOKOj-FQ  2014-12-12 00:44:23, 2015-01-09 00:19:52, 2015...\n",
       "8  --FWWsIwxRwuw9vIMImcQg  2010-09-11 16:28:39, 2010-12-22 21:14:19, 2011...\n",
       "9  --FcbSxK1AoEtEAxOgBaCw  2017-08-18 19:43:50, 2017-10-07 22:38:38, 2017..."
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "df2 = pd.read_csv('/Users/jeffreytaylor/Documents/Emory/CS370/PracticumProject/yelp_dataset/checkin.csv')\n",
    "\n",
    "df2.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('---kPU91CF4Lq2-WlRu9Lw', '2020-03-13 21:10:56, 2020-06-02 22:18:06, 2020-07-24 22:42:27, 2020-10-24 21:36:13, 2020-12-09 21:23:33, 2021-01-20 17:34:57, 2021-04-30 21:02:03, 2021-05-25 21:16:54, 2021-08-06 21:08:08, 2021-10-02 15:15:42, 2021-11-11 16:23:50')\n",
      "('--0iUa4sNDFiZFrAdIWhZQ', '2010-09-13 21:43:09, 2011-05-04 23:08:15, 2011-07-18 22:30:31, 2012-09-07 20:28:50, 2013-03-27 15:57:36, 2013-08-13 00:31:34, 2013-08-13 00:31:48, 2013-09-23 17:39:38, 2013-11-18 06:34:08, 2014-04-12 23:04:47')\n",
      "('--30_8IhuyMHbSOcNWd6DQ', '2013-06-14 23:29:17, 2014-08-13 23:20:22')\n",
      "('--7PUidqRWpRSpXebiyxTg', '2011-02-15 17:12:00, 2011-07-28 02:46:10, 2012-03-11 10:30:02, 2012-04-24 07:07:59, 2012-04-24 07:43:31, 2013-05-25 16:41:10, 2014-05-02 15:49:55, 2014-09-18 02:28:23, 2014-11-10 15:16:43, 2015-09-27 13:18:32')\n",
      "('--7jw19RH9JKXgFohspgQw', '2014-04-21 20:42:11, 2014-04-28 21:04:46, 2014-09-30 14:41:47, 2014-10-23 18:22:28, 2015-04-27 19:55:00, 2015-09-21 12:52:09, 2015-10-01 12:46:16, 2015-10-22 13:35:04, 2016-01-14 12:27:43, 2016-02-01 15:15:07, 2016-02-11 12:22:47, 2016-03-31 23:15:46, 2016-04-11 13:11:34, 2016-05-25 12:44:11, 2016-06-27 15:46:11, 2016-06-30 15:14:28, 2016-07-28 14:15:59, 2016-09-22 20:05:06, 2016-11-16 19:00:11, 2016-12-27 14:36:14, 2017-01-24 20:18:16, 2017-02-21 16:01:49, 2017-03-21 14:08:44, 2017-05-21 16:07:21, 2017-08-07 14:17:39, 2021-06-21 19:59:50')\n",
      "('--8IbOsAAxjKRoYsBFL-PA', '2015-06-06 01:03:19, 2015-07-29 16:50:58, 2015-08-04 00:47:59, 2015-08-07 02:07:52, 2015-08-07 17:25:40, 2015-10-12 21:08:43, 2015-11-13 03:06:00, 2015-11-14 02:35:36, 2015-11-21 00:22:42, 2015-12-01 02:03:11, 2015-12-23 01:19:16, 2015-12-23 20:14:44, 2016-01-09 21:20:39, 2016-01-17 04:11:32, 2016-05-19 01:41:28, 2016-05-30 19:09:07, 2016-09-16 22:58:53, 2016-09-22 17:32:16, 2016-09-23 23:35:31, 2016-09-23 23:41:23, 2016-10-04 17:55:19, 2016-10-28 18:38:36, 2017-06-04 02:11:09, 2017-06-23 19:36:51, 2017-09-26 22:46:41, 2017-10-15 00:44:45, 2017-11-14 20:51:58, 2017-12-02 19:22:33, 2017-12-04 19:28:58, 2018-02-15 20:48:11, 2018-04-30 18:30:36, 2018-07-12 21:34:36')\n",
      "('--9osgUCSDUWUkoTLdvYhQ', '2015-06-13 02:00:57, 2015-07-04 00:44:09, 2015-07-19 23:38:22, 2015-07-26 03:16:22, 2015-08-03 17:32:59, 2015-08-03 17:33:18, 2015-12-13 02:49:34, 2016-02-14 04:15:58, 2016-02-14 14:45:08, 2016-04-30 01:54:26, 2016-07-03 00:05:12, 2017-10-23 02:17:44, 2018-04-28 00:42:09, 2018-05-07 01:31:36, 2018-05-16 00:42:15, 2018-05-16 01:02:48, 2018-05-16 01:03:37, 2018-05-16 01:36:57, 2018-05-16 01:42:02, 2018-05-16 01:53:10, 2018-05-16 01:55:01, 2018-05-16 03:53:55, 2018-12-25 00:50:41, 2019-01-01 01:49:40')\n",
      "('--ARBQr1WMsTWiwOKOj-FQ', '2014-12-12 00:44:23, 2015-01-09 00:19:52, 2015-02-13 21:51:29, 2015-02-28 22:11:18, 2015-03-06 03:10:50, 2015-03-28 20:42:09, 2015-04-03 23:55:06, 2015-04-23 00:21:59, 2015-05-13 22:41:22, 2015-06-27 22:10:25, 2015-07-08 23:53:24, 2015-07-21 21:51:16, 2015-08-12 03:16:41, 2015-08-13 01:42:13, 2015-09-05 23:51:03, 2015-09-10 22:28:40, 2015-10-07 23:16:37, 2015-11-07 21:22:36, 2016-01-17 00:13:47, 2016-01-22 21:07:36, 2016-03-17 23:46:23, 2016-06-30 01:22:21, 2016-07-21 23:22:49, 2016-11-10 00:51:25, 2016-12-21 23:18:43, 2016-12-21 23:20:40, 2017-06-17 23:58:10, 2017-12-02 23:25:19, 2018-03-08 22:26:30, 2018-04-08 00:04:16, 2018-04-20 22:16:48, 2018-05-27 02:49:33, 2018-06-23 01:18:47, 2018-07-22 01:42:39')\n",
      "('--FWWsIwxRwuw9vIMImcQg', '2010-09-11 16:28:39, 2010-12-22 21:14:19, 2011-08-03 16:26:33, 2011-11-21 15:06:32, 2011-12-29 18:54:30, 2012-01-21 22:25:52, 2012-09-16 22:21:21')\n",
      "('--FcbSxK1AoEtEAxOgBaCw', '2017-08-18 19:43:50, 2017-10-07 22:38:38, 2017-10-22 21:22:59, 2017-12-10 18:36:24, 2017-12-15 20:00:14, 2017-12-24 15:17:56, 2018-01-17 19:41:48, 2018-01-23 12:13:34, 2018-02-01 12:16:38, 2018-02-03 13:18:44, 2018-03-03 14:36:50, 2018-03-05 16:01:22, 2018-03-10 13:32:17, 2018-03-25 18:15:08, 2018-04-29 20:35:11, 2018-06-16 20:59:00, 2018-08-03 14:36:39, 2018-09-15 19:20:08, 2018-10-12 12:16:08, 2018-10-21 19:34:49, 2018-10-25 21:06:56, 2019-01-01 18:40:50, 2019-01-12 15:03:41, 2019-01-18 22:49:14, 2019-01-19 18:55:58, 2019-02-06 14:21:26, 2019-02-06 21:24:06, 2019-02-22 14:16:34, 2019-03-09 15:14:10, 2019-03-17 17:23:58, 2019-03-20 19:22:57, 2019-03-30 14:28:58, 2019-05-12 14:11:54, 2019-05-18 20:14:11, 2019-05-23 16:33:16, 2019-05-23 18:55:03, 2019-06-02 18:50:02, 2019-06-15 21:07:19, 2019-06-16 13:48:33, 2019-07-03 13:13:37, 2019-07-23 00:35:46, 2019-07-30 23:41:58, 2019-08-05 15:06:20, 2019-10-15 21:34:41, 2019-10-26 13:32:46, 2019-10-26 14:17:20, 2019-11-23 18:30:26, 2019-12-08 21:44:54, 2019-12-28 15:55:48, 2020-01-11 15:27:28, 2020-01-26 18:20:55, 2020-02-07 16:04:09, 2020-03-07 20:24:45, 2020-03-22 13:42:48, 2020-05-02 14:00:21, 2020-05-09 16:37:03, 2020-05-21 16:45:17, 2020-07-06 18:57:58, 2020-08-01 13:35:33, 2020-08-03 15:15:15, 2020-08-12 14:27:15, 2020-09-19 16:18:50, 2020-10-17 12:43:26, 2020-10-29 14:48:14, 2020-11-08 17:05:57, 2020-12-13 18:43:03, 2021-02-21 14:23:21, 2021-02-28 14:46:06, 2021-03-14 17:11:22, 2021-04-18 19:39:30, 2021-05-09 13:00:18, 2021-05-25 15:22:08, 2021-07-04 15:25:38, 2021-08-01 18:09:31, 2021-08-29 16:56:41, 2021-09-19 14:26:55, 2021-11-12 18:11:18, 2021-12-06 14:43:02, 2021-12-15 12:25:17, 2021-12-19 15:36:55, 2021-12-31 20:00:13, 2022-01-09 18:33:57')\n"
     ]
    }
   ],
   "source": [
    "# Testing SQL Query AI Bot User\n",
    "import psycopg2\n",
    "\n",
    "# Connect to the PostgreSQL server\n",
    "conn = psycopg2.connect(\n",
    "    host=\"envitas-db.cvga08s8c0bm.us-west-1.rds.amazonaws.com\",\n",
    "    database=\"postgres\",\n",
    "    user=\"bot\",\n",
    "    password=\"envitas\"\n",
    ")\n",
    "\n",
    "# Create a cursor object to interact with the database\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Set the search path to 'yelp' schema\n",
    "cur.execute(\"SET search_path TO yelp\")\n",
    "\n",
    "# Execute the query\n",
    "query = \"SELECT business_id, date FROM raw_checkin LIMIT 10;\"\n",
    "cur.execute(query)\n",
    "\n",
    "# Fetch all the results\n",
    "results = cur.fetchall()\n",
    "\n",
    "# Print the results\n",
    "for row in results:\n",
    "    print(row)\n",
    "\n",
    "# Close the cursor and connection\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Photos Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to CSV file: /Users/jeffreytaylor/Documents/Emory/CS370/PracticumProject/yelp_photos/photos.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import csv\n",
    "\n",
    "# Replace 'your_file.json' with the actual filename of your JSON data\n",
    "filename = '/Users/jeffreytaylor/Documents/Emory/CS370/PracticumProject/Data/yelp_photos/photos.json'\n",
    "\n",
    "# Read the JSON data into a pandas DataFrame\n",
    "df = pd.read_json(filename, lines=True)\n",
    "\n",
    "# Function to remove newlines within a string\n",
    "def remove_newlines(text):\n",
    "  return re.sub(r'\\n', '', text)\n",
    "\n",
    "# Apply remove_newlines function to the caption column only\n",
    "df['caption'] = df['caption'].apply(remove_newlines)\n",
    "\n",
    "# Define the filename for the output CSV file\n",
    "output_filename = '/Users/jeffreytaylor/Documents/Emory/CS370/PracticumProject/Data/yelp_photos/photos.csv'\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(output_filename, index=False, quoting=csv.QUOTE_ALL)\n",
    "\n",
    "print(f'Data successfully written to CSV file: {output_filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>photo_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>caption</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>zsvj7vloL4L5jhYyPIuVwg</td>\n",
       "      <td>Nk-SJhPlDBkAZvfsADtccA</td>\n",
       "      <td>Nice rock artwork everywhere and craploads of ...</td>\n",
       "      <td>inside</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HCUdRJHHm_e0OCTlZetGLg</td>\n",
       "      <td>yVZtL5MmrpiivyCIrVkGgA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>outside</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vkr8T0scuJmGVvN2HJelEA</td>\n",
       "      <td>_ab50qdWOk0DdB6XOrBitw</td>\n",
       "      <td>oyster shooter</td>\n",
       "      <td>drink</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pve7D6NUrafHW3EAORubyw</td>\n",
       "      <td>SZU9c8V2GuREDN5KgyHFJw</td>\n",
       "      <td>Shrimp scampi</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>H52Er-uBg6rNrHcReWTD2w</td>\n",
       "      <td>Gzur0f0XMkrVxIwYJvOt2g</td>\n",
       "      <td>NaN</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>wZ29mUm6nKz566j17OBadw</td>\n",
       "      <td>jl38yx7zzMRbg-kOK8NLDw</td>\n",
       "      <td>NaN</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>QRUgAISgYLQJ9SK2yOwomw</td>\n",
       "      <td>-9NmUeTphyS9Lq1o9MACGw</td>\n",
       "      <td>NaN</td>\n",
       "      <td>inside</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>bb7o8kXXXqc-8PWU6_wcuA</td>\n",
       "      <td>RRCgIohWjaeGtlbpcYJBbw</td>\n",
       "      <td>NaN</td>\n",
       "      <td>inside</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mcjlyGuLFJ0t4vDixycCSg</td>\n",
       "      <td>p2J__JQ_mN5lVd7iGXYgGA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>inside</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3ROd5PAQ_0OkmoKWVO06ag</td>\n",
       "      <td>u9vhzYtXpfyvAOAMnyy-Cw</td>\n",
       "      <td>Inside reception</td>\n",
       "      <td>inside</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 photo_id             business_id  \\\n",
       "0  zsvj7vloL4L5jhYyPIuVwg  Nk-SJhPlDBkAZvfsADtccA   \n",
       "1  HCUdRJHHm_e0OCTlZetGLg  yVZtL5MmrpiivyCIrVkGgA   \n",
       "2  vkr8T0scuJmGVvN2HJelEA  _ab50qdWOk0DdB6XOrBitw   \n",
       "3  pve7D6NUrafHW3EAORubyw  SZU9c8V2GuREDN5KgyHFJw   \n",
       "4  H52Er-uBg6rNrHcReWTD2w  Gzur0f0XMkrVxIwYJvOt2g   \n",
       "5  wZ29mUm6nKz566j17OBadw  jl38yx7zzMRbg-kOK8NLDw   \n",
       "6  QRUgAISgYLQJ9SK2yOwomw  -9NmUeTphyS9Lq1o9MACGw   \n",
       "7  bb7o8kXXXqc-8PWU6_wcuA  RRCgIohWjaeGtlbpcYJBbw   \n",
       "8  mcjlyGuLFJ0t4vDixycCSg  p2J__JQ_mN5lVd7iGXYgGA   \n",
       "9  3ROd5PAQ_0OkmoKWVO06ag  u9vhzYtXpfyvAOAMnyy-Cw   \n",
       "\n",
       "                                             caption    label  \n",
       "0  Nice rock artwork everywhere and craploads of ...   inside  \n",
       "1                                                NaN  outside  \n",
       "2                                     oyster shooter    drink  \n",
       "3                                      Shrimp scampi     food  \n",
       "4                                                NaN     food  \n",
       "5                                                NaN     food  \n",
       "6                                                NaN   inside  \n",
       "7                                                NaN   inside  \n",
       "8                                                NaN   inside  \n",
       "9                                   Inside reception   inside  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "df2 = pd.read_csv('/Users/jeffreytaylor/Documents/Emory/CS370/PracticumProject/yelp_photos/photos.csv')\n",
    "\n",
    "df2.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('zsvj7vloL4L5jhYyPIuVwg', 'Nk-SJhPlDBkAZvfsADtccA', 'Nice rock artwork everywhere and craploads of taps.', 'inside')\n",
      "('HCUdRJHHm_e0OCTlZetGLg', 'yVZtL5MmrpiivyCIrVkGgA', '', 'outside')\n",
      "('vkr8T0scuJmGVvN2HJelEA', '_ab50qdWOk0DdB6XOrBitw', 'oyster shooter', 'drink')\n",
      "('pve7D6NUrafHW3EAORubyw', 'SZU9c8V2GuREDN5KgyHFJw', 'Shrimp scampi', 'food')\n",
      "('H52Er-uBg6rNrHcReWTD2w', 'Gzur0f0XMkrVxIwYJvOt2g', '', 'food')\n",
      "('wZ29mUm6nKz566j17OBadw', 'jl38yx7zzMRbg-kOK8NLDw', '', 'food')\n",
      "('QRUgAISgYLQJ9SK2yOwomw', '-9NmUeTphyS9Lq1o9MACGw', '', 'inside')\n",
      "('bb7o8kXXXqc-8PWU6_wcuA', 'RRCgIohWjaeGtlbpcYJBbw', '', 'inside')\n",
      "('mcjlyGuLFJ0t4vDixycCSg', 'p2J__JQ_mN5lVd7iGXYgGA', '', 'inside')\n",
      "('3ROd5PAQ_0OkmoKWVO06ag', 'u9vhzYtXpfyvAOAMnyy-Cw', 'Inside reception', 'inside')\n"
     ]
    }
   ],
   "source": [
    "# Testing SQL Query AI Bot User\n",
    "import psycopg2\n",
    "\n",
    "# Connect to the PostgreSQL server\n",
    "conn = psycopg2.connect(\n",
    "    host=\"envitas-db.cvga08s8c0bm.us-west-1.rds.amazonaws.com\",\n",
    "    database=\"postgres\",\n",
    "    user=\"bot\",\n",
    "    password=\"envitas\"\n",
    ")\n",
    "\n",
    "# Create a cursor object to interact with the database\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Set the search path to 'yelp' schema\n",
    "cur.execute(\"SET search_path TO yelp\")\n",
    "\n",
    "# Execute the query\n",
    "query = \"SELECT * FROM raw_photos LIMIT 10;\"\n",
    "cur.execute(query)\n",
    "\n",
    "# Fetch all the results\n",
    "results = cur.fetchall()\n",
    "\n",
    "# Print the results\n",
    "for row in results:\n",
    "    print(row)\n",
    "\n",
    "# Close the cursor and connection\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reviews Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to CSV file: /Users/jeffreytaylor/Documents/Emory/CS370/PracticumProject/yelp_dataset/review.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import csv\n",
    "\n",
    "# Replace 'your_file.json' with the actual filename of your JSON data\n",
    "filename = '/Users/jeffreytaylor/Documents/Emory/CS370/PracticumProject/Data/yelp_dataset/yelp_academic_dataset_review.json'\n",
    "\n",
    "# Read the JSON data into a pandas DataFrame\n",
    "df = pd.read_json(filename, lines=True)\n",
    "\n",
    "# Define the filename for the output CSV file\n",
    "output_filename = '/Users/jeffreytaylor/Documents/Emory/CS370/PracticumProject/Data/yelp_dataset/review.csv'\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(output_filename, index=False, quoting=csv.QUOTE_ALL)\n",
    "\n",
    "print(f'Data successfully written to CSV file: {output_filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KU_O5udG6zpxOg-VcAEodg</td>\n",
       "      <td>mh_-eMZ6K5RLWhZyISBhwA</td>\n",
       "      <td>XQfwVwDr-v0ZS3_CbbE5Xw</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>If you decide to eat here, just be aware it is...</td>\n",
       "      <td>2018-07-07 22:09:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BiTunyQ73aT9WBnpR9DZGw</td>\n",
       "      <td>OyoGAe7OKpv6SyGZT5g77Q</td>\n",
       "      <td>7ATYjTIgM3jUlt4UM3IypQ</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>I've taken a lot of spin classes over the year...</td>\n",
       "      <td>2012-01-03 15:28:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>saUsX_uimxRlCVr67Z4Jig</td>\n",
       "      <td>8g_iMtfSiwikVnbP2etR0A</td>\n",
       "      <td>YjUWPpI6HXG530lwP-fb2A</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Family diner. Had the buffet. Eclectic assortm...</td>\n",
       "      <td>2014-02-05 20:30:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AqPFMleE6RsU23_auESxiA</td>\n",
       "      <td>_7bHUi9Uuf5__HHc_Q8guQ</td>\n",
       "      <td>kxX2SOes4o-D3ZQBkiMRfA</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Wow!  Yummy, different,  delicious.   Our favo...</td>\n",
       "      <td>2015-01-04 00:01:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sx8TMOWLNuJBWer-0pcmoA</td>\n",
       "      <td>bcjbaE6dDog4jkNY91ncLQ</td>\n",
       "      <td>e4Vwtrqf-wpJfwesgvdgxQ</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Cute interior and owner (?) gave us tour of up...</td>\n",
       "      <td>2017-01-14 20:54:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>JrIxlS1TzJ-iCu79ul40cQ</td>\n",
       "      <td>eUta8W_HdHMXPzLBBZhL1A</td>\n",
       "      <td>04UD14gamNjLY0IDYVhHJg</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>I am a long term frequent customer of this est...</td>\n",
       "      <td>2015-09-23 23:10:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6AxgBCNX_PNTOxmbRSwcKQ</td>\n",
       "      <td>r3zeYsv1XFBRA4dJpL78cw</td>\n",
       "      <td>gmjsEdUsKpj9Xxu6pdjH0g</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Loved this tour! I grabbed a groupon and the p...</td>\n",
       "      <td>2015-01-03 23:21:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>_ZeMknuYdlQcUqng_Im3yg</td>\n",
       "      <td>yfFzsLmaWF2d4Sr0UNbBgg</td>\n",
       "      <td>LHSTtnW3YHCeUkRDGyJOyw</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Amazingly amazing wings and homemade bleu chee...</td>\n",
       "      <td>2015-08-07 02:29:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ZKvDG2sBvHVdF5oBNUOpAQ</td>\n",
       "      <td>wSTuiTk-sKNdcFyprzZAjg</td>\n",
       "      <td>B5XSoSG3SfvQGtKEGQ1tSQ</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>This easter instead of going to Lopez Lake we ...</td>\n",
       "      <td>2016-03-30 22:46:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>pUycOfUwM8vqX7KjRRhUEA</td>\n",
       "      <td>59MxRhNVhU9MYndMkz0wtw</td>\n",
       "      <td>gebiRewfieSdtt17PTW6Zg</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Had a party of 6 here for hibachi. Our waitres...</td>\n",
       "      <td>2016-07-25 07:31:06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                review_id                 user_id             business_id  \\\n",
       "0  KU_O5udG6zpxOg-VcAEodg  mh_-eMZ6K5RLWhZyISBhwA  XQfwVwDr-v0ZS3_CbbE5Xw   \n",
       "1  BiTunyQ73aT9WBnpR9DZGw  OyoGAe7OKpv6SyGZT5g77Q  7ATYjTIgM3jUlt4UM3IypQ   \n",
       "2  saUsX_uimxRlCVr67Z4Jig  8g_iMtfSiwikVnbP2etR0A  YjUWPpI6HXG530lwP-fb2A   \n",
       "3  AqPFMleE6RsU23_auESxiA  _7bHUi9Uuf5__HHc_Q8guQ  kxX2SOes4o-D3ZQBkiMRfA   \n",
       "4  Sx8TMOWLNuJBWer-0pcmoA  bcjbaE6dDog4jkNY91ncLQ  e4Vwtrqf-wpJfwesgvdgxQ   \n",
       "5  JrIxlS1TzJ-iCu79ul40cQ  eUta8W_HdHMXPzLBBZhL1A  04UD14gamNjLY0IDYVhHJg   \n",
       "6  6AxgBCNX_PNTOxmbRSwcKQ  r3zeYsv1XFBRA4dJpL78cw  gmjsEdUsKpj9Xxu6pdjH0g   \n",
       "7  _ZeMknuYdlQcUqng_Im3yg  yfFzsLmaWF2d4Sr0UNbBgg  LHSTtnW3YHCeUkRDGyJOyw   \n",
       "8  ZKvDG2sBvHVdF5oBNUOpAQ  wSTuiTk-sKNdcFyprzZAjg  B5XSoSG3SfvQGtKEGQ1tSQ   \n",
       "9  pUycOfUwM8vqX7KjRRhUEA  59MxRhNVhU9MYndMkz0wtw  gebiRewfieSdtt17PTW6Zg   \n",
       "\n",
       "   stars  useful  funny  cool  \\\n",
       "0      3       0      0     0   \n",
       "1      5       1      0     1   \n",
       "2      3       0      0     0   \n",
       "3      5       1      0     1   \n",
       "4      4       1      0     1   \n",
       "5      1       1      2     1   \n",
       "6      5       0      2     0   \n",
       "7      5       2      0     0   \n",
       "8      3       1      1     0   \n",
       "9      3       0      0     0   \n",
       "\n",
       "                                                text                 date  \n",
       "0  If you decide to eat here, just be aware it is...  2018-07-07 22:09:11  \n",
       "1  I've taken a lot of spin classes over the year...  2012-01-03 15:28:18  \n",
       "2  Family diner. Had the buffet. Eclectic assortm...  2014-02-05 20:30:30  \n",
       "3  Wow!  Yummy, different,  delicious.   Our favo...  2015-01-04 00:01:03  \n",
       "4  Cute interior and owner (?) gave us tour of up...  2017-01-14 20:54:15  \n",
       "5  I am a long term frequent customer of this est...  2015-09-23 23:10:31  \n",
       "6  Loved this tour! I grabbed a groupon and the p...  2015-01-03 23:21:18  \n",
       "7  Amazingly amazing wings and homemade bleu chee...  2015-08-07 02:29:16  \n",
       "8  This easter instead of going to Lopez Lake we ...  2016-03-30 22:46:33  \n",
       "9  Had a party of 6 here for hibachi. Our waitres...  2016-07-25 07:31:06  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "df2 = pd.read_csv('/Users/jeffreytaylor/Documents/Emory/CS370/PracticumProject/yelp_dataset/review.csv')\n",
    "\n",
    "df2.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('XQfwVwDr-v0ZS3_CbbE5Xw', '3', '0', '0', '0', \"If you decide to eat here, just be aware it is going to take about 2 hours from beginning to end. We have tried it multiple times, because I want to like it! I have been to it's other locations in NJ and never had a bad experience. \\n\\nThe food is good, but it takes a very long time to come out. The waitstaff is very young, but usually pleasant. We have just had too many experiences where we spent way too long waiting. We usually opt for another diner or restaurant on the weekends, in order to be done quicker.\", '2018-07-07 22:09:11')\n",
      "('7ATYjTIgM3jUlt4UM3IypQ', '5', '1', '0', '1', \"I've taken a lot of spin classes over the years, and nothing compares to the classes at Body Cycle. From the nice, clean space and amazing bikes, to the welcoming and motivating instructors, every class is a top notch work out.\\n\\nFor anyone who struggles to fit workouts in, the online scheduling system makes it easy to plan ahead (and there's no need to line up way in advanced like many gyms make you do).\\n\\nThere is no way I can write this review without giving Russell, the owner of Body Cycle, a shout out. Russell's passion for fitness and cycling is so evident, as is his desire for all of his clients to succeed. He is always dropping in to classes to check in/provide encouragement, and is open to ideas and recommendations from anyone. Russell always wears a smile on his face, even when he's kicking your butt in class!\", '2012-01-03 15:28:18')\n",
      "('YjUWPpI6HXG530lwP-fb2A', '3', '0', '0', '0', 'Family diner. Had the buffet. Eclectic assortment: a large chicken leg, fried jalapeÃ±o, tamale, two rolled grape leaves, fresh melon. All good. Lots of Mexican choices there. Also has a menu with breakfast served all day long. Friendly, attentive staff. Good place for a casual relaxed meal with no expectations. Next to the Clarion Hotel.', '2014-02-05 20:30:30')\n",
      "('kxX2SOes4o-D3ZQBkiMRfA', '5', '1', '0', '1', \"Wow!  Yummy, different,  delicious.   Our favorite is the lamb curry and korma.  With 10 different kinds of naan!!!  Don't let the outside deter you (because we almost changed our minds)...go in and try something new!   You'll be glad you did!\", '2015-01-04 00:01:03')\n",
      "('e4Vwtrqf-wpJfwesgvdgxQ', '4', '1', '0', '1', \"Cute interior and owner (?) gave us tour of upcoming patio/rooftop area which will be great on beautiful days like today. Cheese curds were very good and very filling. Really like that sandwiches come w salad, esp after eating too many curds! Had the onion, gruyere, tomato sandwich. Wasn't too much cheese which I liked. Needed something else...pepper jelly maybe. Would like to see more menu options added such as salads w fun cheeses. Lots of beer and wine as well as limited cocktails. Next time I will try one of the draft wines.\", '2017-01-14 20:54:15')\n",
      "('04UD14gamNjLY0IDYVhHJg', '1', '1', '2', '1', \"I am a long term frequent customer of this establishment. I just went in to order take out (3 apps) and was told they're too busy to do it. Really? The place is maybe half full at best. Does your dick reach your ass? Yes? Go fuck yourself! I'm a frequent customer AND great tipper. Glad that Kanella just opened. NEVER going back to dmitris!\", '2015-09-23 23:10:31')\n",
      "('gmjsEdUsKpj9Xxu6pdjH0g', '5', '0', '2', '0', \"Loved this tour! I grabbed a groupon and the price was great. It was the perfect way to explore New Orleans for someone who'd never been there before and didn't know a lot about the history of the city. Our tour guide had tons of interesting tidbits about the city, and I really enjoyed the experience. Highly recommended tour. I actually thought we were just going to tour through the cemetery, but she took us around the French Quarter for the first hour, and the cemetery for the second half of the tour. You'll meet up in front of a grocery store (seems strange at first, but it's not terribly hard to find, and it'll give you a chance to get some water), and you'll stop at a visitor center part way through the tour for a bathroom break if needed. This tour was one of my favorite parts of my trip!\", '2015-01-03 23:21:18')\n",
      "('LHSTtnW3YHCeUkRDGyJOyw', '5', '2', '0', '0', 'Amazingly amazing wings and homemade bleu cheese. Had the ribeye: tender, perfectly prepared, delicious. Nice selection of craft beers. Would DEFINITELY recommend checking out this hidden gem.', '2015-08-07 02:29:16')\n",
      "('B5XSoSG3SfvQGtKEGQ1tSQ', '3', '1', '1', '0', \"This easter instead of going to Lopez Lake we went to Los Padres National Forest which is really pretty but if you go to white rock the staff needs to cut down all the dead grass that invades the rock and the water. I would wish the staff would also clean or get rid of the dead grass that's also living by the water. The water is really green and dirty. Los padres national forest staff need to work hard to maintain this forest looking pretty and not like a dumpster. Even Cachuma lake looks like they put a bit more effort.\", '2016-03-30 22:46:33')\n",
      "('gebiRewfieSdtt17PTW6Zg', '3', '0', '0', '0', \"Had a party of 6 here for hibachi. Our waitress brought our separate sushi orders on one plate so we couldn't really tell who's was who's and forgot several items on an order. I understand making mistakes but the restaraunt was really quiet so we were kind of surprised. Usually hibachi is a fun lively experience and our  cook  said maybe three words, but he cooked very well his name was Francisco. Service was fishy, food was pretty good, and im hoping it was just an off night here. But for the money I wouldn't go back.\", '2016-07-25 07:31:06')\n"
     ]
    }
   ],
   "source": [
    "# Testing SQL Query AI Bot User\n",
    "import psycopg2\n",
    "\n",
    "# Connect to the PostgreSQL server\n",
    "conn = psycopg2.connect(\n",
    "    host=\"envitas-db.cvga08s8c0bm.us-west-1.rds.amazonaws.com\",\n",
    "    database=\"postgres\",\n",
    "    user=\"bot\",\n",
    "    password=\"envitas\"\n",
    ")\n",
    "\n",
    "# Create a cursor object to interact with the database\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Set the search path to 'yelp' schema\n",
    "cur.execute(\"SET search_path TO yelp\")\n",
    "\n",
    "# Execute the query\n",
    "query = \"SELECT business_id, stars, useful, funny, cool, text, date FROM raw_review LIMIT 10;\"\n",
    "cur.execute(query)\n",
    "\n",
    "# Fetch all the results\n",
    "results = cur.fetchall()\n",
    "\n",
    "# Print the results\n",
    "for row in results:\n",
    "    print(row)\n",
    "\n",
    "# Close the cursor and connection\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Push CSV Files to SQL Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL Line to Copy Data\n",
    "# \\copy SQL_TABLE_NAME from '/Users/jeffreytaylor/Documents/Emory/CS370/PracticumProject/yelp_dataset/DATA.csv' delimiter ',' CSV HEADER;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hw1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
